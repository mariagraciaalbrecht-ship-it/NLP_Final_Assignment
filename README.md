# NLP_Final_Assignment

This assignment uses the EU_debates database (source:https://huggingface.co/datasets/RJuro/eu_debates) to explore some NLP topics. The database contains interventions/speeches (in text) from different political figures from the EU, delivered and recorded in different languages. From this analysis, I intend to get insights related to the topics that different parties from Spanish-speaking debaters, from 2018 to 2022. I chose these years because of certain important events that took place (e.g. the pandemic in 2020 and the war between Russia and Ukraine in 2022), and considered it could be interesting to see how the topics over time, and to see if different parties usually speak more about certain topics than others. I selected Spanish-speaking debaters to explore how the LLM and BERTopic can handle text in Spanish and then extracting topics in English, and I thought it would be also interesting to see how it would deal with context-specific Spanish issues.

For my extraction process, I first used an LLM to extract topics, the year in which the intervention/speech was carried out, and the party that the speaker belonged to. I realized that the LLM was not going to successfully extract the year and the party from the text because not all debaters would mention their party in their speech, let alone the year in which they were speaking. Then I opted to use a hybrid model which combined the original party and year, and then extract only the topic. I did this because I thought it would be a lot more useful to maintain years and parties in order to construct the knowledge graphs and carry out the network analysis. 
For the network analysis, I chose to identify which of the parties spoke about the widest range of topics, and which were the topics that were most discussed across most parties and also ones that were more unique to a party or a few parties. I used degree centrality in order to achieve this, as the parties with the most degrees would be the ones discussing the widest range of topics, and the topics with more degrees would be the ones discussed among more parties. Because my original plan was to see if there were major differences before and after the pandemic, I then split the data in two groups. One included the years 2018-19 (pre-pandemic), and the other would consider the years 2020 to 2022 (post-pandemic, or those where the pandemic was either appearing or evolving). Then I ran degree centrality on both groups to compare the most discussed topics. For the network analysis, I chose the topics extracted by BERTopic and refined by the LLM, because I thought it would be more "readable" and easier to have fewer, more general topics to map out.

Summary of findings:
I found that S&D was the Spanish political party that discussed the widest range of topics, and that one of the topics that was most discussed ws "EU Politics".
Pre-COVID, the most discussed topics were "Human rights violations", "EU Politics" and "Turkey EU Relations". These two first topics were also among the most discussed post-Covid, along with "Spanish climate solidarity fund" and "EU Ukraine War".

Summary of limitations:
As mentioned before, the LLM is limited when it needs to extract information that might not be present in the text, such as party name and year.
With the hybrid model using the LLM, the LLM labelled each text/speech very specifically. This made it be very precise when I manually checked the accuracy of the topics, but this is less useful when you want to have broader topics. It may be that many of the topics extracted by the LLM were similar and could be grouped together, which would require taking similarity into consideration.
Because BERTopic considers similarity, I thought it would be useful to use BERTopic to assign topic labels to the text and then refine this with the LLM. The limitations were that the topic were too broad, which made the network analysis more difficult because I assume many important topics were hidden inside very broad topics like "EU Politics". I was able to confirm "EU Politics" was very broad and often missed the most important issues discussed in the speeches, when I manually checked the accuracy of the labels.

Requirements:
BERTopic>=0.15.0
openai>=1.0.0
python-dotenv>=1.0.0
pydantic>=2.0.0
pandas>=2.0.0
scikit-learn>=1.3.0
umap-learn>=0.5.0
hdbscan>=0.8.0
sentence-transformers>=2.2.0
networkx>=3.0
holoviews>=1.16.0
bokeh>=3.0.0
datasets>=2.14.0
nltk>=3.8.0
tqdm>=4.65.0
matplotlib>=3.7.0
seaborn>=0.12.0
httpx>=0.24

(Requirements generated by ChatGPT and DeepSeek)



